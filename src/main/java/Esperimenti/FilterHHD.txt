genera:
HDDclassification ----->failed      (1431 files)
                    |
                    --->fullYear    (50450 files)
                    |
                    --->other       (29292 files)

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.util.*;

public class Main {

    public static void main(String[] args) throws IOException {
        final String data_path = Utils.path;
        final int columnsFailure = 3;
        final String failedTag = "$FAILED";

        System.out.println("Data path: " + data_path);

        JavaSparkContext spark_context = new JavaSparkContext(new SparkConf()
                .setAppName("Spark Count")
                .setMaster("local")
        );

        //fix filesystem errors when using java .jar execution
        spark_context.hadoopConfiguration().set("fs.hdfs.impl",
                org.apache.hadoop.hdfs.DistributedFileSystem.class.getName()
        );
        spark_context.hadoopConfiguration().set("fs.file.impl",
                org.apache.hadoop.fs.LocalFileSystem.class.getName()
        );

        JavaPairRDD<String, String> textFile = spark_context.wholeTextFiles(data_path + "Out2_HardDisksFilesOrdered/", 10000);

        JavaPairRDD<String, ArrayList<String[]>> dischi = textFile.mapToPair(fileDisco ->
        {
            String[] tempKey = fileDisco._1().split("/");
            tempKey[0] = tempKey[tempKey.length - 1];//reuse the value at 0 as name

            ArrayList<String[]> giornate = new ArrayList<String[]>();
            for (String giorno : fileDisco._2().split(String.format("%n"))) {
                String[] temp = giorno.split(",");

                if (temp[columnsFailure].compareTo("1") == 0)
                    tempKey[0] = tempKey[0] + failedTag;

                giornate.add(temp);
            }

            return new Tuple2(tempKey[0], giornate);
        });

        dischi.cache();

        dischi.foreach((Tuple2<String, ArrayList<String[]>> HHDfile) ->
        {
            String filename = data_path + "HDDclassification";
            if (HHDfile._1().contains(failedTag))
                filename = filename + "/failed/" + HHDfile._1().replace(failedTag, "");
            else
                //360 because from 2016-06-03 to 2016-06-08 are missing
                if (HHDfile._2().size() >= 360)
                    filename = filename + "/fullYear/" + HHDfile._1();
                else
                    filename = filename + "/other/" + HHDfile._1();

            File fileOutput = new File(filename);
            if (!fileOutput.exists()) {
                fileOutput.createNewFile();
            }
            FileWriter fw = new FileWriter(fileOutput.getAbsoluteFile(), false); // creating fileWriter object with the file
            BufferedWriter bw = new BufferedWriter(fw); // creating bufferWriter which is used to write the content into the file
            for (String[] giorno : HHDfile._2()) {
                for (String valore : giorno) {
                    bw.write(valore + ",");
                }
                bw.write(String.format("%n"));
            }
            bw.close();
        });
    }
}