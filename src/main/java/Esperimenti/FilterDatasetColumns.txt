import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;

public class Main {

    public static void main(String[] args) throws IOException {
        /*frequent columns
        0,date,24471617
        1,serial_number,24471617
        2,model,24471617
        3,capacity_bytes,24471617
        4,failure,24471617
        94,smart_255_raw,24471617       ???????????????????             MEH: i don't know what this is
        6,smart_1_raw,24471593          Read Error Rate                 MEH: often not meaningful as a decimal number
        10,smart_3_raw,24471593         Spin-Up Time                    MEH:
        12,smart_4_raw,24471593         Start/Stop Count                MEH: why? ==> datacenter
        14,smart_5_raw,24471593         Read Channel Margin             Let's try
        16,smart_7_raw,24471593         Seek Error Rate                 MEH: The raw value has different structure for different vendors and is often not meaningful as a decimal number
        20,smart_9_raw,24471593         Power-On Hours                  MEH: why? ==> datacenter
        22,smart_10_raw,24471593        Spin Retry Count                MEH: why? ==> datacenter
        26,smart_12_raw,24471593        Power Cycle Count               MEH: why? ==> datacenter
        58,smart_197_raw,24471593       Current Pending Sector Count    Let's try
        60,smart_198_raw,24471593       Uncorrectable Sector Count      Let's try
        62,smart_199_raw,24471593       UltraDMA CRC Error Count        MEH: not HDD failure
        52,smart_194_raw,24471342       Temperature                     MEH: why? ==> datacenter
        48,smart_192_raw,24428958       Unsafe Shutdown Count           MEH: why? ==> datacenter

        TODO:
        50,smart_193_raw,24157811       SUSPECT!!! RIPROVA TUTTE LE ANALISI (quando sar√† tutto pronto) INCLUDENDO QUESTO!!!!

        */
        final int[] colonneValide = new int[]{0, 1, 2, 3, 4, 14, 58, 60,50};

        final String data_path = Utils.path;
        System.out.println("Data path: " + data_path);

        JavaSparkContext spark_context = new JavaSparkContext(new SparkConf()
                .setAppName("Spark Count")
                .setMaster("local")
        );

        JavaPairRDD<String, String> textFile = spark_context.wholeTextFiles(data_path + "Data", 1000);

        JavaPairRDD<String, String> rows = textFile.mapToPair(file ->
        {
            String[] filename = file._1().split("/");

            String fullFilename = data_path + "filteredColumns/" + filename[filename.length - 1];
            File fileFiltered = new File(fullFilename);
            if (!fileFiltered.exists()) {
                fileFiltered.createNewFile();
            }
            FileWriter fw = new FileWriter(fileFiltered.getAbsoluteFile(), false);
            BufferedWriter bw = new BufferedWriter(fw);

            String[] righe = file._2().split(String.format("\n"));
            for (int i = 0; i < righe.length; i++) {
                String[] valori = righe[i].split(",");

                //clean the last two character to fix a strange behaviour with new "line type of operations"
                //Gaspa remembers the mysterious "^M" on vim, unknow to Java (and to himself also).
                valori[valori.length - 1] = "";
                valori[valori.length - 2] = "";
                for (int j = 0; j < colonneValide.length; j++) {
                    //scrive solo le colonne con indice contenuto in "colonneValide"
                    bw.write(valori[colonneValide[j]] + ",");
                }
                bw.write(String.format("%n"));
            }
            bw.close();
            return new Tuple2(file._1(), "DONE!");
        });

        rows.foreach((Tuple2<String, String> tupla) -> System.out.println(tupla._1() + " " + tupla._2()));
    }
}