import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.PairFlatMapFunction;
import scala.Tuple2;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.lang.*;
import java.util.ArrayList;
import java.util.List;

public class Main {

    public static void main(String[] args) throws IOException {
        final String data_path = Utils.path;

        System.out.println("Data path: " + data_path);

        JavaSparkContext spark_context = new JavaSparkContext(new SparkConf()
                .setAppName("Spark Count")
                .setMaster("local")
        );

        //fix filesystem errors when using java .jar execution
        spark_context.hadoopConfiguration().set("fs.hdfs.impl",
                org.apache.hadoop.hdfs.DistributedFileSystem.class.getName()
        );
        spark_context.hadoopConfiguration().set("fs.file.impl",
                org.apache.hadoop.fs.LocalFileSystem.class.getName()
        );

        /*frequent columns
        4,failure,24471617
        14,smart_5_raw,24471593         Read Channel Margin
        50,smart_193_raw,24157811       Load/Unload Cycle Count
        58,smart_197_raw,24471593       Current Pending Sector Count
        60,smart_198_raw,24471593       Uncorrectable Sector Count

        */

        final int[] colonneValide = new int[]{4, 14, 50, 58, 60};

        JavaRDD<String> textFileLastDay = spark_context.textFile(data_path + "AnalisiFrequenzaValori/lastDay.csv", 10000);

        JavaPairRDD<String, ArrayList<String>> healthy = textFileLastDay.mapToPair(riga ->
        {
            String key = "0";
            String[] valori = riga.split(",");
            ArrayList<String> lista = new ArrayList<>();
            if (valori[4].compareTo("0") == 0) {
                for (int i = 0; i < colonneValide.length; i++)
                    if (valori[colonneValide[i]].compareTo("") == 0)
                        lista.add("0");
                    else
                        lista.add(valori[colonneValide[i]]);

            } else {
                key = "-1";
                for (int i = 0; i < colonneValide.length; i++)
                    lista.add("0");
            }
            return new Tuple2(key, lista);
        });

        JavaPairRDD<String, String> textFile = spark_context.wholeTextFiles(data_path + "Data", 1000);

        JavaPairRDD<String, ArrayList<Tuple2<String, ArrayList<String>>>> failedGroupByDay = textFile.mapToPair(file ->
        {
            String[] dischi = file._2().split(String.format("\n"));
            ArrayList<Tuple2<String, ArrayList<String>>> lista = new ArrayList<>();
            for (String disco : dischi) {
                String[] valori = disco.split(",");
                if (valori[4].compareTo("1") == 0) {
                    ArrayList<String> recordDisco = new ArrayList<>();
                    for (int i = 0; i < colonneValide.length; i++) {
                        if (valori[colonneValide[i]].compareTo("") == 0)
                            recordDisco.add("0");
                        else
                            recordDisco.add(valori[colonneValide[i]]);
                    }
                    lista.add(new Tuple2("1", recordDisco));
                }
            }
            return new Tuple2(1, lista);
        });

        JavaPairRDD<String, ArrayList<String>> failed = failedGroupByDay.flatMapToPair((PairFlatMapFunction<Tuple2<String, ArrayList<Tuple2<String, ArrayList<String>>>>, String, ArrayList<String>>) t -> {
            List<Tuple2<String, ArrayList<String>>> result = new ArrayList<>();

            for (Tuple2<String, ArrayList<String>> lista : t._2()) {
                result.add(new Tuple2(lista._1(), lista._2()));
            }
            return result.iterator();
        });

        List<Tuple2<String, ArrayList<String>>> failedCollected = failed.collect();
        List<Tuple2<String, ArrayList<String>>> healthyCollected = healthy.collect();

        String filename = data_path + "forMATLAB.csv";
        File fileOutput = new File(filename);
        if (!fileOutput.exists()) {
            fileOutput.createNewFile();
        }

        FileWriter fw = new FileWriter(fileOutput.getAbsoluteFile(), false); // creating fileWriter object with the file
        BufferedWriter bw = new BufferedWriter(fw); // creating bufferWriter which is used to write the content into the file

        String[] COLONNE = new String[]{"failure", "Read Channel Margin",
                "Current Pending Sector Count", "Uncorrectable Sector Count",
                "Load_Unload Cycle Count"};
        for (String nome : COLONNE)
            bw.write(nome + ",");
        bw.write(String.format("%n"));

        for (Tuple2<String, ArrayList<String>> record : failedCollected) {
            for (String valore : record._2()) {
                bw.write(valore + ",");
            }
            bw.write(String.format("%n"));
        }

        for (Tuple2<String, ArrayList<String>> record : healthyCollected) {
            for (String valore : record._2()) {
                bw.write(valore + ",");
            }
            bw.write(String.format("%n"));
        }
        bw.close();

        filename = data_path + "forMATLABFailed.csv";
        fileOutput = new File(filename);
        if (!fileOutput.exists()) {
            fileOutput.createNewFile();
        }

        fw = new FileWriter(fileOutput.getAbsoluteFile(), false);
        bw = new BufferedWriter(fw);

        for (String nome : COLONNE)
            bw.write(nome + ",");
        bw.write(String.format("%n"));

        for (Tuple2<String, ArrayList<String>> record : failedCollected) {
            for (String valore : record._2()) {
                bw.write(valore + ",");
            }
            bw.write(String.format("%n"));
        }

        bw.close();

        filename = data_path + "forMATLABHealthy.csv";
        fileOutput = new File(filename);
        if (!fileOutput.exists()) {
            fileOutput.createNewFile();
        }

        fw = new FileWriter(fileOutput.getAbsoluteFile(), false);
        bw = new BufferedWriter(fw);

        for (String nome : COLONNE)
            bw.write(nome + ",");
        bw.write(String.format("%n"));

        for (Tuple2<String, ArrayList<String>> record : healthyCollected) {
            for (String valore : record._2()) {
                bw.write(valore + ",");
            }
            bw.write(String.format("%n"));
        }
        bw.close();
    }
}