import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.PairFlatMapFunction;
import scala.Tuple2;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class Main {

    public static void main(String[] args) throws IOException {
        final String data_path = Utils.path;

        System.out.println("Data path: " + data_path);

        JavaSparkContext spark_context = new JavaSparkContext(new SparkConf()
                .setAppName("Spark Count")
                .setMaster("local")
        );

        //fix filesystem errors when using java .jar execution
        spark_context.hadoopConfiguration().set("fs.hdfs.impl",
                org.apache.hadoop.hdfs.DistributedFileSystem.class.getName()
        );
        spark_context.hadoopConfiguration().set("fs.file.impl",
                org.apache.hadoop.fs.LocalFileSystem.class.getName()
        );

        JavaPairRDD<String, String> textFile = spark_context.wholeTextFiles(data_path + "Data", 400);

        JavaPairRDD<String, ArrayList<Tuple2<String, String>>> failedGroupByDay = textFile.mapToPair(file ->
        {
            String key="-1";
            String[] dischi = file._2().split(String.format("\n"));
            ArrayList<Tuple2<String, ArrayList<String>>> lista = new ArrayList<>();
            for (String disco : dischi) {
                key="0";
                String[] valori = disco.split(",");
                if (valori[4].compareTo("1") == 0) {
                    lista.add(new Tuple2("1", disco));
                }
            }
            return new Tuple2(key, lista);
        });

        JavaPairRDD<String, String> failedDisks = failedGroupByDay.flatMapToPair((PairFlatMapFunction<Tuple2<String, ArrayList<Tuple2<String, String>>>, String, String>) t -> {
            List<Tuple2<String, String>> resultFailed = new ArrayList<>();

            for (Tuple2<String, String> lista : t._2()) {
                resultFailed.add(new Tuple2(lista._1(), lista._2()));
            }
            return resultFailed.iterator();
        });

        List<Tuple2<String, String>> resultFailed = failedDisks.collect();

        String filename = data_path + "AnalisiFrequenzaValori/failedDisks.csv";
        File fileOutput = new File(filename);
        if (!fileOutput.exists()) {
            fileOutput.createNewFile();
        }

        FileWriter fw = new FileWriter(fileOutput.getAbsoluteFile(), false);
        BufferedWriter bw = new BufferedWriter(fw);

        for (Tuple2<String, String> tupla : resultFailed) {
            if (tupla._1().compareTo("-1") != 0) {
                bw.write(tupla._2());
                bw.write(String.format("%n"));
            }
        }
        bw.write(String.format("%n"));
        bw.close();
    }
}